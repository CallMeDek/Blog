# Going deeper with convolutions

Christian Szegedy(Google Inc.), Wei Liu(University of North Carolina, Chapel Hill), 

Yangqing Jia(Google Inc.), Pierre Sermanet(Google Inc.), Scott Reed(University of Michigan), 

Dragomir Anguelov(Google Inc.), Dumitru Erhan(Google Inc.), Vincent Vanhoucke(Google Inc.),

Andrew Rabinovich(Google Inc.)



## Abstract

저자들은 2014년 ImageNet Large-Scale Visual Recognition Challenge 2014(ILSVRC14)에서 객체 분류 및 탐지 분야에 최공 성능을 보인 Inception이라는 Deep CNN 아키텍처를 제안했다. 아키텍처를 신중하게 디자인해서 컴퓨터가 계산하는 양에 큰 부담을 주지 않으면서 아키텍처의 깊이와 넓이를 늘리는 것을 가능하게 했다. ILSVRC14에서 선보인 구현체의 이름이 GoogLeNet인데 22개의 계층을 가지고 있다.



## Introduction

저자들이 주장하길 CNN 덕분에 객체 인식과 탐지 분야에 상당한 진보가 일어났다고 한다. 그런데 이것은 단순히 더 강한 컴퓨팅 파워나 더 많아진 데이터셋 때문이 아니고 주로 네트워크 아키텍처를 개선시키는 아이디어 혹은 알고리즘 같은 것들 때문이라고 한다. 새로운 데이터라고 할 게 없는게 ILSVRC 2014에서 분류나 탐지에 쓰인 데이터가 같은 데이터라고 한다. 저자들이 2014에 제출한 GoogLeNet은 2년전에 Krizhevsky 등이 우승을 차지 했을 때의 구현체보다 12배 적은 모델 파라미터를 사용하면서 더 정확하다고 한다. 특히 객체 탐지에서 얻은 결과는 모델이나 앙상블의 활용 덕분이 아니라 Girshick 등에 의해 고안된 R-CNN 알고리즘 덕분이라고 한다. 

저자들이 말하는 이 연구의 중요점은 모바일이나 임베디드에서의 효율성 - 전력이나 메모리 사용량 같은 -에서 중요성을 가지지 정확도를 좀 더 높이거나 하는데 있는게 아니라고 한다. 가장 주안점을 뒀던 점을 뒀던 부분은 추론 시에 곱하기-더하기 연산을 1.5 billion만큼으로 유지해서 단순히 학술적인 의미만 남기는 것이 아니라 실제 필드에서도 사용될 수 있도록 한 것이라고 한다. 

Inception이라는 이름은 영화 인셉션의 명대사인 "we need to go deeper"와 Lin 등이 제안했던 Network in network에서부터 착안된 것이라고 한다. 여기서 deep이라는 말은 Inception module에서 보여준 wide하게 용량을 늘리는 부분과 단순히 네트워크의 depth를 늘리는 것, 두 가지 관점이 담겨 있다. 



## Related Work

LeNet5는 표준적인 구조 양상을 띄고 있다. 컨볼루션 계층이 쌓여 있고 선택적으로 Contrast normalization(우리가 흔히 하는 평균을 0, 분산이 1인 정규분포를 따르는)나 Max pooling과, 완전 연결 계층들이 그 위에 쌓여 있다. 이런 디자인이 MNIST, CIFAR 혹은 ImageNet 분류 문제에서 최고의 성적을 냈다. 특히 ImageNet 같은 거대한 데이터셋에서는 계층의 숫자나 계층 자체에 대한 크기를 늘리면서 Dropout 같은 기법으로 과적합 문제를 해결하고는 했다고 한다. 

Max pooling 계층 때문에 공간 정보의 손실이 야기될 수 있다는 염려에도 불구하고 Krizhevsky 등의 Advances in Neural Information Processing Systems와 같은 CNN 아키텍처가 객체 위치 추정, 객체 탐지, 인간 포즈 추정 같은 분야에 성공적으로 적용되었다. Serre 등은 뇌과학자의 영장류 시각 피질 모델에 영감을 받아서, Inception 모듈과 유사하게 다양한 크기의 객체를 다루기 위해 각기 다른 크기의, 학습을 하지 않는 Gabor 필터들을 사용했다. 그러나 Inception 모듈은 전자와 다르게 학습이 가능하고 전체 네트워크에서 한 번만 등장하는 것이 아니라, 여러번 쓰여서 22 계층의 딥러닝 모델을 이룬다. 

Lin 등에 의해서 제안된 Network-in-Network에서는 신경망 네트워크의 표현력을 증대시키는데 초점을 맞췄다. 이걸 CNN에 적용할 때, 1x1 컨볼루션 계층과 ReLU를 적용한 방법과 동일하게 볼 수 있는데 이렇게 하면 CNN 파이프라인에 쉽게 합쳐질 수 있었다. 저자들이 말하는 1x1 컨볼루션의 두 가지 목적이 있는데 가장 주요한 이유는 계산상의 병목 현상-이 현상 때문에 네트워크의 크기가 제한될 수 있다-을 제거하기 위함이다. 이 연산으로 단순히 깊이를 늘리는것 뿐만 아니라 아주 약간의 성능 저하를 감수하고 넓이를 늘릴 수 있다. 

Girshick에 의해서 고안된 Regions with Convolutional Neural Networks는 전체적인 탐지 과정을 두 분으로 나눈다. 첫 번째로 카테고리에 상관 없이 객체가 있을법한 영역법한 픽셀 그룹과 색깔 같은 저차원의 신호들을 이용하는 것이고 두 번째로 이렇게 찾아낸 위치의 객체들의 카테고리를 확인하기 위한 CNN 분류기를 사용하는 것이다. 저자들도 비슷한 파이프라인은 탐지 작업을 위해서 차용했다. 



## Motivation and High Level Considerations

네트워크의 크기나 넓이를 늘리는 것은 두 가지 주요한 단점이 있다. 

큰 사이즈의 네트워크는 더 많은 모델 파라미터를 보유하게 되고 과적합하는 경향을 보인다. 특히 레이블링된 훈련셋의 숫자가 적을 때 더 그렇다. 이 점은 네트워크 성능에서 주요한 병목점이 될 수 있다. 아래 그림과 같이 레이블링을 하기 위해서 전문가가 필요한 상황에서는 레이블링을 하는 과정이 힘들고 번거로운 작업이 될 수 있다.

![](./Figure/Going_deeper_with_convolutions1.JPG)

두번째로 일정하게 증가하는 네트워크 사이즈에 반해 컴퓨팅 자원의 사용이 급격하게 증가한다는 점이다. 예를 들어서 두 컨볼루션 계층이 연결되어 있다면, 필터의 숫자는 일정하게 증가하는데, 이를 연산하는 과정은 거의 두 배로 증가한다. 만약에 추가된 부분이 비효율적으로 사용된다면(대부분의 가중치가 0으로 수렴한다던지) 많은 잘못된 연산으로 낭비되고 있는 것이다. 그러므로 현업에서는 컴퓨팅 예산이 한정되어 있고 결과를 개선하는게 목적이라 할지라도 무분별하게 네트워크의 사이즈를 늘리는 것에 컴퓨팅 자원을 효율적으로 분배하는 것이 필요하다. 

위 두 가지 사항을 해결하는 근본적인 방법은 최종적으로 계층의 뉴런을 완전 연결하는 것에서 드문 드문 연결하는 것으로 바꾸는 것이다. Arora 등의 획기적인 성과로 인해 이런 방법이 확고한 이론적 뒷받침을 확보할 수 있었다. 이들의 연구 결과가 암시하는 바는 데이터 셋의 확률 분포가 아주 크고 딥하지만 드물게 연결되어 있는 신경망 네트워크에 의해서 표현될 수 있다면 마지막 계층의 활성화 함수의 상관 관계 통계 수치를 분석하고 높은 연관성이 있는 출력 뉴런 끼리 클러스터링을 함으로서 계층 마다 최적의 토폴로지를 구축할 수 있다는 것이다. 이 주장은 엄격한 수학적 증명이 필요하긴 하나, Hebbian 원칙처럼 - neurons that fire together, wire together - 그렇게 엄격하지 않아도 실제로 적용가능 한다는 점을 시사한다. 

균일하지 않은 희소 데이터 행렬에서의 숫자 연산에 대해서 당시에 컴퓨팅 인프라들은 비효율적이었다. 산술 연산은 100배 가까이 줄어들었을지 몰라도 Lookup에 의한 오버헤드나 캐시 미스가 지배적이므로 희소 행렬로 전환하는 것이 효용이 없어진다. 이런 격차(Dense vs Sparse)는 점점 더 벌어진다. 왜냐하면 CPU나 GPU 하드웨어의 세부사항을 이용하여 만든, 극도로 빠른 밀집 데이터 곱셉을 가능하게 하는, 점점 개선되고 튜닝되는 숫자 관련 라이브러리들 때문이다. 그리고 균일하지 않은 희소 모델은 좀 더 세련된 엔지니어링과 컴퓨팅 인프라를 필요로 한다. 대부분의 비전 머신러닝 시스템들은 컨볼루션의 장점을 활용해 공간과 관련된 도메인에서 희소성을 이용한다. 하지만 컨볼루션 또한 이전 계층에서의, 밀집 연결된 패치들의 집합으로 구현된다. ConvNet이 원래 학습 방법을 개선하고 대칭성을 깨트리기 위해 랜덤한 희소 연결 테으블을 사용했는데 이런 추세가 병렬 컴퓨팅을 최적화 하기 위해서 다시 완전 연결로 되돌아왔다. 구조의 균일함과 많은 필터 큰 배치 사이즈가 효율적인 밀집 연산을 활용 가능하게 한다. 