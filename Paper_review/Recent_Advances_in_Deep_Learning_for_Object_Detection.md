# Recent Advances in Deep Learning for Object Detection



Xiongwei Wu(School of Information System, Singapore Management University),

Doyen Sahoo(School of Information System, Singapore Management University),

Steven C.H. Hoi(School of Information System, Singapore Management University, Salesforce Research Asia)



arXiv: 1908.03673v1







## 0. Abstract

객체 탐지는 주어진 이미지 안의 특정 타겟 클래스의 객체의 정확한 위치를 찾아내어 그에 부합하는 클래스 레이블을 부여하는 것을 목적으로 한다. 최근에 딥러닝 기반 객체 분류가 큰 성공을 거두어 객체 탐지에서도 딥러닝을 사용하는 방법이 활발하게 연구 되었다. 이 연구에서는 현재 존재하는 객체 탐지 프레임워크를 시스템적으로 분석하고 다음과 같은 부분을 조사했다. 1) Detection components 2) Learning strategies 3) Application & benchmarks. 또한 탐지 성능에 영향을 주는 여러 요소들에 대해서도 다루었다(Detector architectures, Feature learning, Proposal generation, Sampling strategies, etc.). 마지막에서는 딥러닝 기반의 객체 탐지 연구가 나아가야할 방향에 대해 다루었다.

Keywords: Object Detection, Deep Learning, Deep Convolutional Neural Networks







## 1. Introduction

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_1.JPG)

(a)와 같은 **Image classification **은 주어진 이미지 안에서 객체를 어떤 카테고리로 분류하는 데에 목적이 있다.

(b)와 같은 **Object detection** 은 카테고리 분류 뿐만 아니라 바운딩 박스로 객체의 위치를 예측하는 작업도 포함한다.

(c)와 같은 **Semantic segmentation** 은 Object detection과 같이 객체를 분류하는 것이 아니라 픽셀 단위로 분류를 진행함으로서 같은 카테고리의 다른 객체를 구별하지 않는다. 

(d)왁 같은 **Instance Segmentation** 은 (b)와 (c)의 개념이 합쳐진다. 이미지 안의 객체들에 대해서 픽셀 단위로 마스킹을 하되, 각각 다르게 분류한다.  (b)에서 바운딩 박스로 Localization 하는 것 대신에 픽셀로 Localization을 한다. 

딥러닝 이전의 객체 탐지 파이프 라인은 세가지 단계로 이루어졌다.

1. Proposal generation -  이 단계에서는 이미지 안에서 객체를 탐지하고 있을만한 위치를 찾는다. 이 위치를 **Regions of interest(ROI)** 라고 한다. 가장 직관적인 방법은 전체 이미지를 슬라이딩 윈도우 방식으로 스캔하는 것인데 다양한 크기의, 각기 다른 가로 세로 비율을 가진 객체들의 정보를 획득하기 위해서 입력 이미지들은 각기 다른 크기로 조정되고 다양한 크기의 윈도우가 이미지를 스캔하기 위해 사용된다.
2. Feature vector extraction - 이 단계에서는 이미지의 각 위치에서 슬라이딩 윈도우를 통해 고정된 크기의 특징 벡터들이 얻어진다. 이 벡터들은 각 지역의 특징 정보를 포함하고 있다. 또한 이 벡터들은 조도, 회전, 크기 변화에 큰 영향을 받지 않는다는 것을 보여준 SIFT(Scale Invariant Feature Transform), Haar, HOG(Histogram of Gradients), SURF(Speeded Up Robust Features) 같은 low-level visual descriptors에 의해 인코딩된다.
3. Region classification - 이 단계에서 지역 분류기들은 관심 지역을 레이블링하는 법을 배운다. 여기에는 SVM(Support Vector machines)가 적은 훈련 세트에도 좋은 성능을 보여서 사용되었다. 특별히 bagging, cascade learning, adaboost 같은 분류 기술들이 탐지 정확도를 개선하기 위해서 이 단계에서 사용되었다.

이런 방법들은 Pascal VOC 데이터 셋에서 인상적인 결과를 달성했다. 

 DPMs는 deformable loss로여러 모델들을 통합하고 학습한다. 그리고 각각의 훈련을 위해 그 안에 잠재되어 있는 SVM으로 심각하게 부정적인 포인트들을 캐낸다. 

그러나 위의 방법들은 다음과 같은 이유에 의해서 한계점을 보였다.

- Proposal generation 과정 중에서, 엄청나게 많은 수의 불필요한 제안들이 생성되었고 이는 분류 과정에서 많은 수의 false positive들을 낳았다. 게다가 윈도우 크기는 수동적으로 경험에 의해서 디자인되었기 때문에 실제 객체에 잘 들어 맞지 않았다.
- Feature descriptor들은 low level visual cues 기반으로 사람이 직접 만들어야 했기 때문에 복잡한 문맥에서는 대표격이라고 할 수 있는 정보들을 잡아내는데 어려움이 있었다.
- 탐지 파이프라인의 각 단계들은 각자 따로 디자인되고 최적화 되었기 때문에 전체 시스템 관점에서의 최적화를 달성하기 어려웠다. 

깊은 컨볼루션 신경망이 이미지 분류에서 큰 성공을 일으키고 객체 탐지 영역에서도 딥러닝 기반의 기술들이 주목할만한 진보를 달성했다.  특히 앞선 전통적인 방법들을 월등히 능가했다.  

한가지의 이미지 분류를 위한 계층적이고 공간 불변의(이미지가 변형되어도 그 이미지로 인식하는 것 - e.g. 고양이 사진을 90도 만큼 회전해도 그 고양이 사진으로 인식) 모델을 구축하기 위한 시도는 Fukushima에 의해 제안된 neocognitron이다. 그러나 이 방법은 지도 학습을 위한 효율적인 최적화 기술이 부재했다. 

이 모델을 기반으로 하여 Lecun 등은 CNN을, 역전파를 통한 SGD(Stochastic gradient descent) 으로 최적화했고 숫자 인식에서 경쟁력 있는 성능을 보여줬다. 

그러나 DCNN은 그 후에 SVM에게 주도권을 내줬는데 그 이유로 다음과 같은 이유들이 있었다.

- 주석이 달린(레이블링 된) 데이터의 부족이 과적합을 일으킴.
- 제한적인 컴퓨팅 자원들(성능 부족).
- SVM와 비교했을 때 이론적 뒷받침이 부족.

2009년에 Jia 등이 획득한 ImageNet이라는 1.2M의 고화질의  주석이 달린 데이터를  대량으로 획득한 덕에 딥러닝 모델을 훈련하는 것이 가능해졌고 GPU 클러스터등의 병렬처리 컴퓨팅시스템의 발전했다. 이로 인해 2012년에 Krizhevsky 등은 ImageNet 데이터 셋으로 크고 깊은 CNN 모델을 훈련시킬 수 있었고 ILSVRC(Large Scale Visual Recognition Challenge)에서 다른 방법들과 비교했을때 상당한 개선 사항을 보여줬다. 

딥러닝(CNN) 기반의 기술이 기존의 방법들과 비교했을 때 가지는 장점은 다음과 같다.

- 훈련 데이터의 저차원 픽셀 단위부터 고차원 의미 정보까지의 계층적 특징 표현들을 자동으로 생성해낸다. 그리고 복잡한 문맥에서도 보다 차별적인 표현 역량을 보여준다. 
- 더 많은 데이터가 있을 때 기존의 방법들은 개선의 여지를 보여줄 수 없는데 반해 딥러닝 기반 기술들은 더 나은 특징 표현을 가능하게 한다.
- 시작부터 끝까지 종단간의 최적화를 가능하게 한다.

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_2.JPG)

현재의 딥러닝 기반 객체 탐지 프레임워크들은 크게 두 가지 범주로 나눌 수 있다.

- R-CNN(Region-based CNN)과 그 아종들과 같은 Two-stage 그룹 - 2단계 탐지기들은 제안 생성기를 통해 제안 세트들을 만들어 내고 각각의 제안들에서 특징들을 추출해낸다. 그리고 나서 제안된 지역들의 카테고리를 분류기를 통해 예측한다.  2단계 탐지기들은 보통 1단계 탐지기들보다 더 좋은 탐지 성능을 보인다.
- YOLO(You only look once)와 그 아종들과 같은 One-stage 그룹 - 1단계 탐지기들은 지역 분류 단계 없이 특징 맵의 각 지역들의 객체들에 대해 곧바로 분류 작업을 수행한다.  1단계 탐지기들은 시간적인 측면에서 효율적이므로 실시간 객체 탐지에 이용 가능하다.

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_3.JPG)



## 2. Problem Settings

객체 탐지는 객체 분류와 같은 인식 과정과 위치 회귀와 같은 위치 측정 과정을 포함한다. 하나의 객체 탐지기는 이미지 안에서 배경으로부터 특정 타겟 클래스의 객체를 구별할 필요가 있다. 또한 각 객체의 카테고리와 위치를 정확하게 맞춰야 한다. 바운딩 박스나 픽셀 마스크가 이런 타겟 객체의 위치를 측정하기 위해서 예측된다.

예를 들어 우리가 N개의, 주석이 달린 이미지 {x1, x2, …, xN}을 가지고 있다고 가정하자. i번째 이미지 xi에는 C라는 카테고리 집합 중에 하나의 카테고리에 속하는 Mi 개의 객체들이 존재한다. 

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_4.JPG)

각각의 c(i, j)는 카테고리 집합 C의 원소이고 i번째 이미지의 j번째 객체의 카테고리를 나타낸다. b(i, j)는 i번째 이미지 xi의 j번째 객체의 바운딩 박스 혹은 픽셀 마스크를 나타낸다. 탐지기 f는 θ에 의해서 매개변수화 된다. xi 이미지의 예측 값 yi_pred는 yi와 같은 형식을 갖는다.

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_5.JPG)

손실 함수 *ℓ* 탐지기를 최적화 하기 위해 다음과 같이 세팅된다.

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_6.JPG)

Softmax loss, focal loss 같은 손실 함수들은 탐지기의 최종 성능에 영향을 끼친다. 

위치 측정의 질을 평가 하기 위해서 객체들과 예측 값들 사이의 **Intersection-over-union(IoU)** 라는 계량법이 사용된다. 

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_7.JPG)

b_gt는 바운딩 박스 혹은 픽셀 마스크의 실제 값을 나타낸다.  IoU의 한계값 Ω은 예측 값이 객체를 밀접하게 덮는지 결정하기 위해서 세팅된다.  객체 탐지에서는 예측 값이 성공적으로 위치 측정을 했는지를 다음과 같이 나타낸다.

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_8.JPG)

일반적인 객체 탐지 문제에서 카테고리 예측을 평가하기 위해서 mean average precision(mAP)가 사용된다. 

탐지 정확도와 더불어 추론 시간도 중요한 요소 중에 하나이다. 추론 시간을 평가 하기 위한 단위로 **Frame per second(FPS)** - (1초당 얼마나 많은 이미지들의 처리가 이루어지는 지)가 사용된다. 보통 20 FPS이면 실시간 탐지기로 여겨 진다.



## 3. Detection Components

### 3.1 Detection Settings

- Vanilla object detection(bbox-level localization) - 객체의 위치를 사각형 바운딩 박스로 측정한다. 따라서 바운딩 박스 주석이 필요하고 평가시에는 예측 바운딩 박스 영역과 실제 바운딩 박스 영역 사이의 IoU가 성능을 측정하기 위해 계산된다. 
- Instance segmentation은 바운딩 박스와는 전혀 다르게 세팅이 되는데 사각형 바운딩 박스 대신 픽셀 단위로 각각의 의미적 객체들은 마킹한다. 정확한 픽셀 레벨의 예측을 위해서 Instance segmentation은 (픽셀에 대해) 공간적으로 잘못 할당되는 것에 민감하므로 더 정확한 공간 정보 처리를 요구한다. 평가는 픽셀 마킹에 대해 IoU 계산이 이루어진다는 것을 제외하고는 바운딩 박스에서의 형식과 동일하다.

### 3.2 Detection Paradigms

|                     | Advantage                                                | Disadvantage                |
| ------------------- | -------------------------------------------------------- | --------------------------- |
| Two-stage detectors | 상대적으로 높은 탐지 성능                                | 추론 속도가 낮다            |
| One-stage detectors | 실시간 객체 탐지에 사용가능할 <br>정도로  높은 추론 속도 | 상대적으로 빈약한 탐지 성능 |

#### 3.2.1 Two-stage Detectors

1.  Proposal generation - 이 단계에서 탐지기는 객체를 포함하고 있을 만한 지역을 확인한다. 높은 재현율(Recall)로 지역을 제시함으로서 이미지 안의 모든 객체들이 적어도 하나쯤은 각 지역에 포함되도록 한다.
2. Making predictions for these proposals - 이 단계에서는 딥러닝 기반 모델이 이 지역들에 대해 레이블링을 한다. 지역은 미리 정의된 레이블 중 하나이거나 배경이라는 레이블이 달린다. 추가적으로 지역 생성기(추천기)에 의해서 제안된 원래의 측정된 위치가 조정될 수 있다. 

다음은 주요 Two-stage 탐지기들이다.

- R-CNN - R-CNN은 2014년에 Girshick 등에 의해 제안된 최초의 Two-stage 탐지기 중 하나이다. 기존의 전통적인 방법의 탐기지 중 하나인 SegDPM가 VOC2010에 40.4%의 mAP를 보였다면 R-CNN은 53.7%의 개선된 성능을 보였다.  R-CNN은 다음과 같은 3단계의 처리 과정을 가진다.

  1.  Selective Search에 의해서 거의 2,000개의 지역들 생성. 이 중에서 배경임을 쉽게 알 수 있는 지역들은 버려짐.
  2. 각 지역들이 어떤 고정된 크기로 잘려지거나 조정되고나서 DCNN에 의해서 하나의 특징 벡터(예를 들어서 4,096 크기)로 인코딩된다. 
  3. one-vs-all SVM 분류기에 의해서 레이블링되고 바운딩 박스 회귀 처리기들이  원래의 지역들이 실제 객체에서 딱 맞도록  입력으로 들어온 인코딩된 특징들을 사용하는 법을 배운다. 

  딥러닝 신경망이 전통적인 특징 디스크럽터들과 다른 점은 신경망의 각 계층에서 다른 크기의 정보를 잡아내고 계층적인 특징들을 생산해내어 분류 작업을 위한 강인하고 차별적인 특징들을 생산한다는 것이다. 

  R-CNN에서는 전이 학습의 이점을 활용하기 위해서 ImageNet 데이터셋으로 미리 학습된 컨볼루션 계층의 가중치들로 CNN의 가중치를 초기화 한다.  그리고 나서 탐지 작업을 위해 완전 연결 계층만 재 학습된다. 그 다음 전체 시스템이 종단 간으로 미세 조정된다. 전이 학습을 이용함으로서 컨볼루션 모델을 학습시키기 전에 쉽게 확인할 수 있는 부정적인 지역들(배경)을 제거함으로서 분류 성능을 개선시키고 학습 시간을 줄일 수 있다.

  R-CNN은 2가지의 치명적인 단점을 가지고 있다.

  1.  각 지역의 추출된 특징들이 지역마다 개별적으로 추출되고 지역마다 공유되지 않기 때문에 반복적인 계산을 해야하고 이는 훈련과 테스트 하는데 엄청난 시간 낭비를 초래한다. 
  2. R-CNN의 각 3가지 과정들이 유기적으로 연결되어 있지 않으므로 시스템 종단간의 최적화를 할 수 없어 시스템 전체적인 최적의 솔루션을 얻기 어렵다.
  3. Selective Search가 저차원 수준의 시각 정보에 의지하기 때문에 복잡한 문맥에서 높은 질의 지역을 생산하기 어렵다
  4. GPU의 이점을 활용하기 어렵다.

- SPP-net - R-CNN의 성능을 가속화하고 특징들을 좀 더 잘 추출하기 위해서 He 등은 Spatial pyramid matching(SPM)에서 영감을 얻은 SPP-net을 제안했다. 각 제안 지역들을 자르고 CNN 모델에 입력으로 넣는 대신에 SPP-net에서는 DCNN에 의해서 전체 이미지에 대한 특징 맵을 계산하고 Spatial Pyramid Pooling(SPP) 계층에 의해서 고정된 크기의 특징 맵에 대한 특징 벡터를 추출해낸다. SPP는 다양한 크기의 N by N 격자로 특징 맵을 나누고 나서 격자의 각 칸에 대하여 풀링을 수행하고 나서 특징 벡터에게 전달한다. 이렇게 전달된 특징 벡터들은 각 지역에 대한 시각 정보를 위해 연결된다(Concatenated). 만들어진 특징 맵들은 지역에 대한 SVM 분류기와 바운딩 박스 회귀 조정기에 전달된다. R-CNN과 비교하면 이미지들이나 이미지의 각 지역들이 크기 자체가 조정되는 일 없이 다양한 크기와 가로세로 비율로 처리가 될 수 잇으므로 정보 손실이나 원치 않는 기하학적 왜곡이 덜 발생된다. SPP-net도 다음과 같은 단점을 가진다.

  1.  시스템의 각 단계들이 여전히 나누어져 있고, 따라서 종단간의 최적화를 진행할 수 없다.
  2.  추출된 특징들을 저장하기 위한 캐시 메모리가 필요하다.
  3.  SPP 층이 컨볼루션 층에 역전파를 하지 않으므로 SPP 이전 층의 모델 파라미터는 동결된다. 이것은 결과적으로 딥러닝 중추 구조(DCNN)의 학습 능력을 제한한다. 

- Fast R-CNN - Girshick 등이 제안한 Fast R-CNN은 SPP-net의 단점을 커버하는 멀티 태스크 학습 탐지기이다. Fast R-CNN은 역시 전체 이미지에서 특징 맵을 계산하고 그 특징 맵에서 고정된 크기의 지역 특징들을 추출한다. SPP-net과는 다르게 지역 특징들을 추출하는 ROI Pooling을 사용한다. ROI Pooling은 SPP의 특수한 경우로서 SPP가 다양한 크기의 N by N 격자를 만들어 냈다면 이와는 반대로 한 가지 크기의 N by N 격자를 만들어내고(특징 맵이 고정된 갯수로 분할됨) 컨볼루션 커널에 역전파를 한다. 특징 추출 이후에 특징 벡터들은 일련의 완전 연결 계층을 거쳐 두 가지 병렬로 연결된(분류 계층(cls), 회귀 계층(reg)) 출력 층으로 전달된다. 분류 계층에서는 C+1(C가지의 클래스와 배경 클래스) 클래스에 관한 소프트맥스 확률을 만들어 내고 회귀 계층에서는 바운딩 박스를 조정하기 위한 4가지의 실수 파라미터를 인코딩한다. Fast R-CNN에서는 특징들을 저장하기 위한 여분의 캐시 메모리가 필요하지 않고 시스템 종단 간의 최적화가 가능하다. Fast R-CNN은 다음과 같은 단점을 가진다.

  - 지역 제안을 만들어내는데 여전히 전통적인 방법(Selective Search, Edge Boxes) - (저차원의 시각 신호를 기반으로 하는)에 의지하고 있기 때문에 데이터 주도적인 방법으로 학습을 할 수 없다. 
  
- Faster R-CNN - Fast R-CNN의 문제를 커버하기 위해서 지역 제안 생성기 부분을 변경함(Region Proposal Network(RPN)). 이 지역 제안 생성기는 지도 학습 방법으로 학습된다. RPN은 완전 컨볼루션 네트워크이고 각기 다른 사이즈의 이미지를 입력으로 받아 특징 맵의 각 부분의 객체가 있을법한 제안들을 생성해낸다. 이 네트워크는 특징 맵을 n by n 슬라이딩 윈도우로 스캔하고 각 부분에 대해 특징 벡터를 만들어낸다. 이렇게 만들어진 특징 벡터들은 병렬적으로 연결되어 있는 출력 브런치에 입력으로 들어가게 된다(지역 제안이 객체인지 아닌지 분류하는 객체 분류 층과 바운딩 박스 회귀 층).  그런 다음 최종적으로 객체를 레이블링하고 바운딩 박스의 위치를 측정하는 계층에 도달한다.  RPN은 Fast R-CNN에 삽입 가능하고 지역 제안 생성이 데이터 주도적인 방법으로 될 수 있게 한다. Faster R-CNN은 GPU 상에서 5FPS의 속도로 예측을 만들어 낼 수 있다. Faster R-CNN은 다음과 같은 문제를 갖고 있다. 

  - 입력 이미지의 특징 맵을 계산하고 서로 다른 지역간의 특징 추출 계산을 공유하는 지역 특징들을 추출해내긴 하나, 계산이 지역 분류 과정에서는 공유되지 않으므로 각각 따로 일련의 완전 연결 계층을 거쳐야할 필요가 있다. 만약에 이미지가 수백만의 후보들을 가지고 있다면 그만큼의 여분의 계산이 더 필요하고, 단순히 완전 연결 계층을 제거한다면 탐지 성능의 급격한 성능을 가져온다. 왜냐하면 딥러닝 네트워크가 후보들의 공간 정보를 줄여야 하기 때문이다.
  - 최종 예측을 만들어 내는데 한 개의 깊은 층 특징 맵만을 사용한다. 이것은 서로 다른 크기의 객체들을 탐지 하기 어렵게 하는데 구체적으로 작은 객체들을 탐지하는데 어렵게 만든다. 깊은 컨볼루션 네트워크에서의 특징 표현 관점에서 깊은 층은 의미적인 정보에는 강력하나 공간 정보에는 약하고 얇은 층에서는 그 반대이다.  

- R-FCN - Dai 등은 Region-based Fully Convolutional Networks를 제안했는데 여기서는 지역 분류 과정에서 계산 노력을 서로 공유한다. R-FCN은 Position Sensitive Score Map이라고 하는 각 클래스들에 대한 각각의 위치 정보를 인코딩한다. 그리고 나서 Position Sensitive ROI Pooling(P-SROI Pooling)에서 타겟 지역들의 각각의 위치를 인코딩한 것으로 공간 인식 지역 특징들을 추출한다. 추출된 특징 벡터들은 공간 정보를 유지하므로 지역별 완전 연결 계층 연산이 없어도 Faste-r R-CNN에 경쟁력 있는 결과를 달성했다. 

- FPN - Lin 등은 (깊은 컨볼루션 네트워크에서의 특징 표현 관점에서 깊은 층은 의미적인 정보에는 강력하나 공간 정보에는 약하고 얇은 층에서는 그 반대)와 같은 속성을 이용해서 Feature Pyram-id Networks라는 깊은 층과 얇은 층의 특징을 합성하여 객체 탐지가 각기 다른 크기의 특징 맵에서 가능하도록 하는 것을 제안했다. 핵심은 얇은 층에서의 공간 정보에 대한 강점과 깊은 층에서의 풍부한 의미적 정보를 강화하는 것이다. FPN은 다양한 크기의 객체를 탐지하는데 진보를 이룩하여 비디오 탐지, 인간의 자세 인식과 같은 분야에서 널리 사용되었다. 

- Instance segmentation - 대부분의 Instance segmentation 알고리즘들은 vanilla 객체 탐지 알고리즘에서 확장되었다. 초기에는 segment 후보들을 생성하고 Fast R-CNN으로 각 segment들을 분류했다. 나중에 Dai등이 MNC라고 하는 멀티 스테이지 알고리즘을 제안했다. 이는 전체 탐지 프레임워크를 여러 단계로 나누고 바운딩 박스 후보들로부터 segmentation 마스크를 예측했다. 그리고 나서 지역 분류기에 의해 카테고리화되었다. 이런 초기의 작업들은 여러 단계에 걸쳐 바운딩 박스와 마스크 예측을 수행했다. 

- Mask R-CNN - He 등은 후보들의 바운딩 박스와 세그멘테이션 마스크의 예측을 병렬적으로 수행하는 Mask R-CNN을 제안했다. 

- Mask Scoring R-CNN - Mask R-CNN을 기반으로 Huang 등은 마스크의 품질을 인식하는 프레임워크를 제안했다. 이것은 예측된 마스크들의 질을 학습하고 마스크의 질과 마스크의 confidence 점수 사이의 에러를 계량화했다. 

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_9.JPG)




#### 3.2.2 One-stage Detectors

Two-stage 탐지기들과 다르게 One-stage 탐지기들은 별개의 후보 생성기나 후보 생성을 학습하는 과정이 존재하지 않는다. 이 탐지기들은 이미지의 모든 지역에 잠재적으로 객체들이 존재할 것으로 가정하고 각 지역을 특정 객체나 배경으로 분류한다. 

- OverFeat - Sermanent 등은 DCNN을 완전 컨볼루션 객체 탐지 안에 포함시켜 객체 탐지를 수행하는 OverFeat을 제안했다. 객체 탐지를 여러 지역 분류 문제로 볼 수 있으므로 OverFeat은 임의의 입력을 위해서 원래의 분류기를, 마지막 완전 연결계층을을 1x1 컨볼루션 층으로 보는 탐지기로 확장시켰다. 분류 네트워크는 입력의 각 지역에 객체의 존재 유무를 가리키는 예측들의 격자를 출력했다. 객체를 확인하고 나서 바운딩 박스 회귀 조정기는 분류기의 DCNN와 같은 특징들에 기반하여 예측된 지역들을 조정하는 법을 학습했다. 다양한 크기의 객체들을 탐지하기 위해서 입력 이미지는 다양한 크기로 조정되어 네트워크의 입력으로 들어갔다. 그리고 나서 여러 크기의 예측들이 합쳐졌다. OverFeat은 컨볼루션 레이어를 사용해서 각 지역들을 포개는 계산을 공유함으써 R-CNN과 비교하여 속도적인 면에서 강점을 보였다. 그리고 오직 한 번의 순전파만 있으면 됐다. 

  - 분류기와 회귀 조정기는 각자 따로 최적화 될 수 밖에 없었다. 

  

- YOLO - Redmon 등은 실시간 탐기지인 YOLO(You Only Look Once)를 개발했다. YOLO는 객체 탐지를 회귀 문제로 보고 전체 이미지를 정해진 정해진 숫자의 격자 칸으로 나누었다(7 x 7 격자 등). 각 셀은 한 개 이상의 객체들의 존재 유무를 탐지하기위한 후보로 여겨졌다.  원래의 구현에서 각 칸은 (최대) 두개 객체들의 중심점을 포함했다. 각 칸에 대해서 예측은 다음과 같은 정보로 이루어져 있었다. 그 지역에 객체가 포함되어 있는지, 바운딩 박스의 좌표 정보와 크기(넓이와 높이), 객체의 클래스 정보. 전체적인 프레임워크는 단일 네트워크로 이루어져 있고 지역 후보생성 과정을 생략함으로서 종단 간의 최적화 방법을 가능하게 했다. YOLO는 기본적으로 45 FPS의 예측을 만들어 낼 수 있었고 좀 더 단순한 중추 구조에 따라서 155 FPS의 처리 속도까지 도달했다. YOLO는 다음과 같은 문제가 있었다.

  - 주어진 한 개의 지역에 대해 최대 2개의 객체만 탐지 가능 하기 때문에 작은 객체들이나 뭉쳐 있는 객체들은 탐지 하기 어려웠다. 
  - 마지막 특징 맵만 예측에 사용되었기 때문에 다양한 크기 혹은 가로 세로 비율의 객체를 예측하기에는 적합하지 않았다. 

  

- SSD - 2016년에 Liu 등은 YOLO 한계들을 다루는 Single-Shot Multibox Detector(SSD)를 제안했다. SSD 또한 이미지를 격차 형태로 나누긴 하지만 YOLO에서 고정된 격자 칸들을 예측하는 것과는 달리 각 격자 칸에, 바운딩 박스의 출력 공간을 나누기 위한 다양한 크기와 가로 세로 비율의 앵커들의 집합이 생성된다. 각 앵커들은 회귀 조정기에 학습된 4개의 오프셋 값으로 조정되고 분류기에 의해 C+1개의 카테고리적 확률이 할당된다. SSD는 여러 특징 맵에서 객체들을 예측했는데 각 특징 맵은 그것의 수용체에 따라서 특정 크기의 객체를 탐지하는 역할을 했다. 큰 객체들을 탐지하고 수용체 영역을 늘리기 위해서 몇몇의 여분의 컨볼루션 특징 맵이 원래의 중추 아키텍처에 추가되었다.  전체 네트워크는 종단간의 훈련 중에 모든 예측 맵들의 분류 손실과 위치 손실들의 가중치 합으로 최적화 되었다. 최종 예측은 각기 다른 특징 맵들의 모든 탐지 결과를 합쳐서 생성되었다.  훈련 중의 그래디언의 많은 부분을 차지하는 부정적인 후보들을 걸러내기 위해서 강력한 부정 후보 추출(mining)이 탐지기를 훈련시키기 위해 사용되었다. 집약적인 데이터 증대도 탐지기의 정확도를 개선하기 위해 적용되었다.  SSD는 Faster R-CNN과 비교해서도 견줄만한 탐지 정확도를 달성했고 실시간에 가까운 추론 능력을 보였다. 

  

- RetinaNet - One-stage 탐기기에서는 판별하기 쉬운 부정 샘플들을 걸러내기 위한 후보 생성 과정 없을 때 생기는 전경(객체)과 배경 사이의 클래스 불균형 문제는 심각한 문제이다. Lin 등은 이런 불균형 문제를 좀 더 유연하게 대처할 수 있는 RetinaNet를 제안했다. RetinaNet은 focal loss(문제가 있어보이는 손실에 좀 더 집중하는)라고 하는, 판별하기 쉬운 부정 샘플들의 그래디언트들을 단순히 버리기 보다 억제하는 손실을 사용했다. 여기에 Feature pyramid networks로 각기 다른 레벨의 특징 맵에서 다양한 크기의 객체를 탐지했다. 여기서 제안된 focal loss는 기존의 강력한 부정 추출 전략(Hard negative mining strategy)를 압도적으로 능가했다. 

  

- YOLOv2 - Redmon 등은 실시간 추론 속도를 유지하면서 탐지 성능을 상당히 개선시킨 YOLOv2라고 하는 YOLO의 개선된 버전을 제안했다. YOLOv2는 ImageNet의 고화질 이미지들(224x224 부터 448x448 까지)로 미리 훈련된 좀 더 강력한 DCNN 중추 아키텍처를 사용했다. 그래서 이 학습된 가중치들은 미세 조정된 정보를 잡아내는데 민감했다. SSD의 앵커 전략에 영감을 받아서 YOLOv2는 세팅을 수동으로 하는 것 대신에 훈련 데이터로부터 k-means 클러스팅을 통해 더 나은 앵커 우선순위를 정의했다. 이것은 위치 측정에서 최적화의 어려움을 줄이는데 도움이 됐다. 또, Batch Normalization 계층과 여러 크기의 훈련 기술들을 추가함으로서 YOLOv2는 당시에 최상의 결과를 도출했다. 

  

- CornerNet - 이전의 접근법들은 탐지기를 훈련시키기 위해서 앵커 박스들을 수동으로 디자인하는 것을 요구했다. 나중에 몇 개의 앵커 없는 탐지기들이 개발되었는데 이 탐지기들의 목적은 객체를 앵커에 맞추기 위해 노력하기 보다 바운딩 박스의 핵심들을 예측하는데 목적을 두었다. Law와 Deng은 앵커 없는 탐지기 중 하나인 CornerNet을 제안했는데 이 탐지기는 객체를 모서리들의  쌍으로 탐지했다.  각 위치에서 특징 맵, 클래스 히트맵, 임베딩 쌍과 모서리 오프셋등을 예측했다.  클래스 히트맵들은 모서리가 될법한 확률을 계산했고 모서리 오프셋들은 모서리 위치를 회귀하는데 사용됐다. 임베딩 쌍들은 같은 객체에 속하는 모서리 쌍들을 그룹 짓는 역할을 했다. 수동으로 디자인된 앵커들에 의존하는 것 없이 CornerNet은 MSCOCO 데이터 셋에서 상당한 성능을 보여줬다. 후에 키포인트 기반의 One-stage 탐지기들의 아종이 나오기도 했다. 

  

![](./Figure/Recent_Advances_in_Deep_Learning_for_Object_Detection_10.JPG)
