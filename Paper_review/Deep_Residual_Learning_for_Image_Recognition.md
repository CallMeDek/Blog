# Deep Residual Learning for Image Recognition

Kaiming He(Microsoft Research), Xiangyu Zhang(Microsoft Research), Shaoqing Ren(Microsoft Research), Jian Sun(Microsoft Research)



## Abstract

딥러닝 모델은 깊이가 길어질수록 훈련 시키기 어렵다. 저자들이 주장하길, 저자들은 이 연구에서 잔차의 개념을 적용한 깊은 신경망 모델 학습 방법을 제시해서 아주 깊은 모델도 학습하기 쉽도록 했다고 한다. 모델의 각 계층의 잔차와 계층으로의 입력을 이용해서 이를 구현했다. 이를 바탕으로 VGG net 계열보다 8배 깊은 152 계층의 모델을 생성했는데도 오히려 복잡도는 더 낮다고 주장한다. 이런 잔차 네트워크의 앙상블 모델이 ImageNet 테스트 셋으로 평가했을 때 3.57%의 에러를 달성했고, ILSVRC 2015 분류 대회에서 1등을 차지했다. 

객체 탐지 분야에서도 모델의 깊이가 중요한 역할을 차지한다고 저자들은 생각했고, 깊이와 관련된 부분만 연구에 적용했을 때, COCO 객체 탐지 데이터셋에서 그 전의 연구보다 28%의 개선됨을 보였다. 저자들의 연구 성과로 다수의 분류 및 탐지 대회에서 수상을 했다고 한다.



## Introduction

DCNN은 이미지 분류 문제에서 하나의 패러다임이 되었다. 이미지 특징들의 Low/Mid/High 단계의 특징들이 합쳐지고 네트워크 안의 분류기들은 종단간으로 협력하게 된다. 특히 계층을 쌓아 올림으로써 이미지에서 추출하는 특징들이 풍부해지는데, ImageNet 대회에서 우수한 성적을 거둔 연구들이 깊은 신경망 모델을 사용했다는 사실을 볼 때, 이 가정이 사실임을 알 수 있다. 

그런데 정말 네트워크에서 계층을 쌓는 것이 쉬울까하는 의문이 들었을 때, 한 가지 문제가 되는 것은 기울기 소실/폭팔 이라는 문제이다. 이 문제가  모델이 훈련을 시작하고 나서 어떤 특정한 수준으로 성능이 수렴하는 것을 방해한다. 다행히 이 부분은 초기화의 정규화, 훈련 중간의 정규화 같은 방법으로 어느정도 해결이 되었다. 그래서 수십개의 계층이 쌓여 있어도 SGD와 역전파로 모델 성능의 수렴이 가능해졌다. 

저자들이 연구를 진행하다가 성능 저하 문제를 발견했다. 네트워크 깊이가 깊어질수록 정확도의 개선이 이루어 지지 않고 정체되어 있다가 급격하게 하락하는 것이다. 그런데 이것은 과적합에 의한 결과가 아니었다. 

![](./Figure/Deep_Residual_Learning_for_Image_Recognition1.JPG)

이런 성능 저하 현상으로 모든 시스템이 최적화하기 쉽다는 것은 아니라는 것을 알 수 있다. 

저자들은 이 연구에서 잔차 학습 프레임워크를 도입하여 이런 성능 저하 문제를 해결하고자 했다. 그래서 계층들이 이런 잔차 매핑을 수행할 수있도록 구축했다. 계층들이 입력 데이터로 출력 데이터를 도출할 수 있도록 하는 원래의 어떤 작업을 H(x)라고 했을 때, 그 쌓여 있는 비선형 계층들의 각 매핑을 F(x) := H(x) - x로 바꿨다. 따라서 원래의 매핑은 F(x)+x로 변경된다. 

F(x)+x는 신경망에서 순전파 시에 Shortcut connection으로 구현될 수 있다. 

![](./Figure/Deep_Residual_Learning_for_Image_Recognition2.JPG)

이 연구에서 Shortcut은 입력 데이터를 가지고, 중간 계층들의 F(x) 연산을 건너뛰고 항등 매핑 연산을 수행해서 결과에 더하는 작업을 수행한다. 항등 매핑 연산을 수행하기 때문에 추가적인 모델 파라미터나 컴퓨팅 작업이 필요하지 않다. 또, 학습 시에 SGD optimization 정책을 따르는 역전파를 네트워크에서 어떤 부분을 수정할 필요없이 여전히 종단간으로 수행할 수 있다. 그리고 Caffe같은 보통의 라이브러리로 쉽게 구현이 가능하다.

저자들은 ImageNet에서 저자들이 가정하고 구현한 프레임워크로 여러가지 포괄적인 실험을 진행했다. 그리고 도출된 결과는 

- 깊은 잔차 네트워크가 최적화하기 쉬운데 반해 평범한 네트워크는 깊이가 깊어질수록 높은 훈련 에러율을 보인다는 점
- 깊은 잔차 네트워크는 깊이가 깊어질때의 정확도가 개선되는 효과를 누릴 수 있다는 점

CIFAR-10 데이터셋서도 이런 결과를 생성함으로 단순히 특정 데이터셋에만 적용되는 것이 아니라는 점을 확인했다. 100개 이상의 층에서 만족할만한 결과를 내었고 1000개 이상의 층에서도 실험을 해봤다고 한다.



## Related Work

본문 참조.



