# 나이브 베이즈 분류기

**나이브 베이즈(Naive bayes)** 분류기는 선형 모델과 매우 유사하나. LogisticRegression 혹은 LinearSVC 같은 선형 분류기보다 훈련 속도가 빠르지만 일반화 성능이 조금 떨어진다.

나이브 베이브 분류기는 각 특성을 개별로 취급하여 파라미터를 학습하고 각 특성에서 클래스별 통계를 단순하게 취합한다.

scikit-learn에 구현된 나이브 베이즈 분류기는 다음과 같다.

- GaussianNB - 연속적인 데이터에 적용. 클래스별로 각 특성의 표준편차와 평균을 저장하고 예측 시에는 데이터 포인트를 클래스의 통계값과 비교하여 가장 잘 맞는 클래스를 예측값으로 한다.
- BernoulliNB - 이진 데이터에 적용. 각 클래스의 특성 중 0이 아닌 것이 몇개인지 센다.
- MultinomialNB - 카운트 데이터(예를 들어 문장에 나타난 단어의 횟수)에 적용. 클래스별 특성의 평균을 계산하고 예측할 때는 GaussianNB와 유사하다.

BernoulliNB, MultinomialNB은 대부분 텍스트 데이터를 분류할 때 사용한다. 예측 공식이 선형모델과 같으나 coef의 경우 기울기가 아니라 특성 카운트 수를 로그 변환한 형태이고 intercept는 클래스 카운트 수를 로그 변환한 형태이다. 



##### 장단점과 매개변수

MultinomialNB와 BernoulliNB는 모델의 복잡도를 조절하는 alpha 매개변수 하나를 가지고 있는데 alpha가 주어지면 모든 특성에 양의 값을 가진 가상의 데이터 포인트를 alpha 갯수만큼 추가한다. 즉 alpha가 크면 모델의 복잡도가 낮아지고 과적합 확률을 줄인다. alpha 값이 성능 향상에 크게 기여하지는 않으나 어느정도 정확도를 개선하는데 도움을 줄 수 있다. 

GaussianNB은 주로 고차원인 데이터셋에 사용하고 다른 두 모델은 텍스트 같은 희소한 데이터를 카운트하는데 사용된다. 0이 아닌 특성이 비교적 많은 데이터 세트에서는 MultinomailNB가 BernoulliNB보다 성능이 높다.

나이브 베이즈 모델과 선형 모델은 상당히 유사하기 때문에 장단점도 비슷하다. 훈련과 예측 속도가 빠르고 훈련과정을 이해하기 쉽다. 희소한 고차원 데이터에서 잘 작동하고 매개변수에 민감하지 않다. 선형 모델로 학습 시간이 오래 걸리는 매우 큰 데이터 세트에 나이브 베이즈 모델을 시도해 볼만 하다.